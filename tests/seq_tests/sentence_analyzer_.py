import os
import sys
import warnings
import re
import argparse
from typing import List, Dict, Tuple, Optional
import json
import time
from collections import defaultdict

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from datasets import load_dataset

from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM,
    pipeline, set_seed
)
import torch


plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


class LocalLLMSentenceAnalyzer:
    """ë¡œì»¬ LLM ë¬¸ì¥ ê¸¸ì´ ë¶„ì„ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, verbose: bool = True):
        """ì´ˆê¸°í™”"""
        self.dataset = None
        self.sentences_data = []
        self.requests_data = []
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.current_model_name = None
        self.verbose = verbose
        
        # ì¶”ì²œ ëª¨ë¸ ëª©ë¡
        self.recommended_models = {
            "korean": [
                "skt/kogpt2-base-v2",  # 125M
                "kakaobrain/kogpt",    # 6B
                "nlpai-lab/kullm-polyglot-5.8b-v2",  # 5.8B
            ],
            "english_small": [
                "microsoft/DialoGPT-small",    # 117M
                "microsoft/DialoGPT-medium",   # 345M
                "facebook/blenderbot-400M-distill",  # 400M
                "google/flan-t5-small",        # 80M
                "google/flan-t5-base",         # 250M
                "EleutherAI/gpt-neo-125M",     # 125M
            ],
            "english_large": [
                "microsoft/DialoGPT-large",    # 762M
                "facebook/blenderbot-1B-distill",   # 1B
                "google/flan-t5-large",        # 780M
                "EleutherAI/gpt-neo-1.3B",     # 1.3B
                "microsoft/GODEL-v1_1-base-seq2seq",  # 220M
            ]
        }
    
    def log(self, message: str):
        """ë¡œê·¸ ì¶œë ¥ (verbose ëª¨ë“œì¼ ë•Œë§Œ)"""
        if self.verbose:
            print(message)
    
    def load_local_model(self, model_name: str, device: str = "auto", use_cache: bool = True):
        """ë¡œì»¬ LLM ëª¨ë¸ ë¡œë“œ"""
        self.log(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        try:
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            if device == "auto":
                device = "cuda" if torch.cuda.is_available() else "cpu"
            
            self.log(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                del self.tokenizer
                del self.pipeline
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.log("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                use_fast=True,
                cache_dir="./model_cache" if use_cache else None
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ì„¤ì •
            model_kwargs = {
                "cache_dir": "./model_cache" if use_cache else None,
                "torch_dtype": torch.float16 if device == "cuda" and torch.cuda.is_available() else torch.float32,
                "low_cpu_mem_usage": True,
            }
            
            # T5 ê³„ì—´ ëª¨ë¸ì€ Seq2Seq, ë‚˜ë¨¸ì§€ëŠ” CausalLM
            if "t5" in model_name.lower() or "godel" in model_name.lower():
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name, **model_kwargs
                ).to(device)
                
                # Seq2Seq ëª¨ë¸ìš© íŒŒì´í”„ë¼ì¸
                self.pipeline = pipeline(
                    "text2text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=0 if device == "cuda" else -1,
                    torch_dtype=model_kwargs["torch_dtype"]
                )
                self.log("ğŸ“‹ ëª¨ë¸ íƒ€ì…: Text-to-Text Generation (Seq2Seq)")
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name, **model_kwargs
                ).to(device)
                
                # CausalLM ëª¨ë¸ìš© íŒŒì´í”„ë¼ì¸
                self.pipeline = pipeline(
                    "text-generation",
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=0 if device == "cuda" else -1,
                    torch_dtype=model_kwargs["torch_dtype"]
                )
                self.log("ëª¨ë¸ íƒ€ì…: Text Generation (CausalLM)")
            
            self.current_model_name = model_name
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            num_params = self.model.num_parameters()
            self.log(f"ğŸ§  ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,} ({num_params/1e6:.1f}M)")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GPU ì‚¬ìš© ì‹œ)
            if device == "cuda" and torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
                self.log(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f}GB / {memory_total:.1f}GB")
            
            self.log(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def load_dataset(self, dataset_name: str, subset: str = None, split: str = "train", 
                    sample_size: int = 1000, language: str = None):
        """ë°ì´í„°ì…‹ ë¡œë“œ"""
        if not DATASETS_AVAILABLE:
            print("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        self.log(f"ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {dataset_name}")
        
        try:
            # ë°ì´í„°ì…‹ ë¡œë“œ
            load_kwargs = {}
            if subset:
                load_kwargs['name'] = subset
            
            self.log("ğŸ”„ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            self.dataset = load_dataset(dataset_name, split=split, **load_kwargs)
            
            self.log(f"ğŸ“¥ ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(self.dataset):,}ê°œ")
            
            # ì–¸ì–´ í•„í„°ë§ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
            if language and 'lang' in self.dataset.column_names:
                original_size = len(self.dataset)
                self.dataset = self.dataset.filter(lambda x: x.get('lang') == language)
                self.log(f"ğŸŒ ì–¸ì–´ í•„í„°ë§ ({language}): {len(self.dataset):,}ê°œ (ì›ë³¸: {original_size:,}ê°œ)")
            
            # ìƒ˜í”Œ í¬ê¸° ì œí•œ
            if len(self.dataset) > sample_size:
                indices = np.random.choice(len(self.dataset), sample_size, replace=False)
                self.dataset = self.dataset.select(indices)
                self.log(f"ğŸ“¦ ëœë¤ ìƒ˜í”Œë§: {sample_size:,}ê°œ ë°ì´í„° ì„ íƒ")
            
            self.log(f"âœ… ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {len(self.dataset):,}ê°œ í•­ëª©")
            
            return True
            
        except Exception as e:
            print(f"ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def generate_responses(self, prompts: List[str], max_length: int = 100, 
                          num_return_sequences: int = 1, temperature: float = 0.7,
                          top_p: float = 0.9, do_sample: bool = True) -> List[str]:
        """ë¡œì»¬ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.pipeline:
            print("ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”")
            return []
        
        self.log(f"ğŸ¯ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
        
        responses = []
        set_seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        start_time = time.time()
        
        for i, prompt in enumerate(prompts):
            try:
                if self.verbose:
                    print(f"ğŸ“ {i+1}/{len(prompts)}: ìƒì„± ì¤‘... ", end="", flush=True)
                
                # ìƒì„± ì„¤ì •
                generation_kwargs = {
                    "max_length": max_length,
                    "num_return_sequences": num_return_sequences,
                    "temperature": temperature,
                    "top_p": top_p,
                    "do_sample": do_sample,
                    "pad_token_id": self.tokenizer.eos_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                }
                
                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ìƒì„±
                if "t5" in str(type(self.model)).lower() or "godel" in str(type(self.model)).lower():
                    # Seq2Seq ëª¨ë¸
                    result = self.pipeline(prompt, **generation_kwargs)
                    generated_text = result[0]['generated_text']
                else:
                    # CausalLM ëª¨ë¸
                    generation_kwargs["max_length"] = len(self.tokenizer.encode(prompt)) + max_length
                    result = self.pipeline(prompt, **generation_kwargs)
                    # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì œê±°
                    generated_text = result[0]['generated_text'][len(prompt):].strip()
                
                responses.append(generated_text)
                
                if self.verbose:
                    print("ì™„ë£Œ")
                
            except Exception as e:
                if self.verbose:
                    print(f"ì‹¤íŒ¨: {e}")
                responses.append("ìƒì„± ì‹¤íŒ¨")
        
        total_time = time.time() - start_time
        self.log(f"âœ… ì „ì²´ ì‘ë‹µ ìƒì„± ì™„ë£Œ! (ì´ {total_time:.1f}ì´ˆ)")
        
        return responses
    
    def split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        sentence_pattern = r'[.!?]+\s+|[ã€‚ï¼ï¼Ÿ]+\s*|[\n\r]+'
        sentences = re.split(sentence_pattern, text.strip())
        sentences = [s.strip() for s in sentences if s.strip() and len(s) > 1]
        return sentences
    
    def calculate_sentence_lengths(self, sentences: List[str], unit='characters') -> List[int]:
        """ë¬¸ì¥ ê¸¸ì´ ê³„ì‚°"""
        if unit == 'characters':
            return [len(s) for s in sentences]
        elif unit == 'words':
            return [len(s.split()) for s in sentences]
        elif unit == 'tokens':
            if self.tokenizer:
                lengths = []
                for s in sentences:
                    try:
                        tokens = self.tokenizer.encode(s, add_special_tokens=False)
                        lengths.append(len(tokens))
                    except:
                        lengths.append(len(s.split()))
                return lengths
            else:
                return [len(s.split()) + len(re.findall(r'[^\w\s]', s)) for s in sentences]
        else:
            raise ValueError("unitì€ 'characters', 'words', ë˜ëŠ” 'tokens' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    
    def analyze_existing_data(self, text_field: str = None, unit='characters'):
        """ê¸°ì¡´ ë°ì´í„°ì…‹ ë¶„ì„"""
        if not self.dataset:
            print("ë¨¼ì € ë°ì´í„°ì…‹ì„ ë¡œë“œí•´ì£¼ì„¸ìš”")
            return None
        
        self.log(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°ì…‹ ë¶„ì„ ({unit} ê¸°ì¤€)")
        
        all_sentences = []
        request_sentence_counts = []
        request_avg_lengths = []
        processed_count = 0
        
        # ë°ì´í„° ì²˜ë¦¬
        for i, item in enumerate(self.dataset):
            # í…ìŠ¤íŠ¸ í•„ë“œ ìë™ ì°¾ê¸°
            text = ""
            if text_field and text_field in item:
                text = item[text_field]
            else:
                # ìë™ í…ìŠ¤íŠ¸ í•„ë“œ íƒì§€
                for field_name in ["text", "content", "response", "answer", "message", "output"]:
                    if field_name in item and isinstance(item[field_name], str):
                        text = item[field_name]
                        break
                
                # ì—¬ì „íˆ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê¸´ ë¬¸ìì—´ í•„ë“œ ì‚¬ìš©
                if not text:
                    for key, value in item.items():
                        if isinstance(value, str) and len(value) > 10:
                            text = value
                            break
            
            if text and isinstance(text, str) and text.strip():
                sentences = self.split_into_sentences(text)
                if sentences:
                    sentence_lengths = self.calculate_sentence_lengths(sentences, unit)
                    
                    all_sentences.extend(sentence_lengths)
                    request_sentence_counts.append(len(sentences))
                    if sentence_lengths:
                        request_avg_lengths.append(np.mean(sentence_lengths))
                    
                    processed_count += 1
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if self.verbose and (i + 1) % 100 == 0:
                print(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {i+1}/{len(self.dataset)} ({processed_count}ê°œ ìœ íš¨)")
        
        # í†µê³„ ê³„ì‚°
        if all_sentences:
            stats = {
                'total_sentences': len(all_sentences),
                'total_requests': len(request_sentence_counts),
                'processed_items': processed_count,
                'avg_sentence_length': np.mean(all_sentences),
                'median_sentence_length': np.median(all_sentences),
                'std_sentence_length': np.std(all_sentences),
                'min_sentence_length': min(all_sentences),
                'max_sentence_length': max(all_sentences),
                'avg_sentences_per_request': np.mean(request_sentence_counts),
                'avg_request_avg_length': np.mean(request_avg_lengths) if request_avg_lengths else 0
            }
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
            print(f"  â€¢ ì²˜ë¦¬ëœ í•­ëª©: {stats['processed_items']:,} / {len(self.dataset):,}")
            print(f"  â€¢ ì´ ë¬¸ì¥ ìˆ˜: {stats['total_sentences']:,}")
            print(f"  â€¢ í‰ê·  ë¬¸ì¥ ê¸¸ì´: {stats['avg_sentence_length']:.2f} {unit}")
            print(f"  â€¢ ì¤‘ê°„ê°’: {stats['median_sentence_length']:.2f} {unit}")
            print(f"  â€¢ í‘œì¤€í¸ì°¨: {stats['std_sentence_length']:.2f} {unit}")
            print(f"  â€¢ ë²”ìœ„: {stats['min_sentence_length']} - {stats['max_sentence_length']} {unit}")
            print(f"  â€¢ ìš”ì²­ë‹¹ í‰ê·  ë¬¸ì¥ ìˆ˜: {stats['avg_sentences_per_request']:.2f}")
            
            # ë°ì´í„° ì €ì¥
            self.sentences_data = all_sentences
            self.requests_data = {
                'sentence_counts': request_sentence_counts,
                'avg_lengths': request_avg_lengths
            }
            
            return stats
        else:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def analyze_generated_responses(self, prompts: List[str], unit='characters', 
                                  max_length: int = 100, temperature: float = 0.7,
                                  show_responses: bool = False):
        """ë¡œì»¬ LLMìœ¼ë¡œ ìƒì„±í•œ ì‘ë‹µ ë¶„ì„"""
        if not self.pipeline:
            print("ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”")
            return None, []
        
        self.log(f"ğŸ¯ ìƒì„±ëœ ì‘ë‹µ ë¶„ì„ ({unit} ê¸°ì¤€)")
        
        # ì‘ë‹µ ìƒì„±
        responses = self.generate_responses(
            prompts, 
            max_length=max_length, 
            temperature=temperature
        )
        
        # ì‘ë‹µ ë¶„ì„
        all_sentences = []
        request_sentence_counts = []
        request_avg_lengths = []
        
        if show_responses:
            print(f"\nğŸ“ ìƒì„±ëœ ì‘ë‹µë“¤:")
            print("-" * 50)
        
        for i, (prompt, response) in enumerate(zip(prompts, responses)):
            if show_responses:
                print(f"\n{i+1}. í”„ë¡¬í”„íŠ¸: {prompt}")
                print(f"   ì‘ë‹µ: {response}")
            
            sentences = self.split_into_sentences(response)
            sentence_lengths = self.calculate_sentence_lengths(sentences, unit)
            
            all_sentences.extend(sentence_lengths)
            request_sentence_counts.append(len(sentences))
            if sentence_lengths:
                request_avg_lengths.append(np.mean(sentence_lengths))
        
        # í†µê³„ ê³„ì‚° ë° ì¶œë ¥
        if all_sentences:
            stats = {
                'total_sentences': len(all_sentences),
                'total_requests': len(prompts),
                'avg_sentence_length': np.mean(all_sentences),
                'median_sentence_length': np.median(all_sentences),
                'std_sentence_length': np.std(all_sentences),
                'min_sentence_length': min(all_sentences),
                'max_sentence_length': max(all_sentences),
                'avg_sentences_per_request': np.mean(request_sentence_counts),
                'avg_request_avg_length': np.mean(request_avg_lengths) if request_avg_lengths else 0
            }
            
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"  â€¢ ì´ ë¬¸ì¥ ìˆ˜: {stats['total_sentences']:,}")
            print(f"  â€¢ í‰ê·  ë¬¸ì¥ ê¸¸ì´: {stats['avg_sentence_length']:.2f} {unit}")
            print(f"  â€¢ ì¤‘ê°„ê°’: {stats['median_sentence_length']:.2f} {unit}")
            print(f"  â€¢ í‘œì¤€í¸ì°¨: {stats['std_sentence_length']:.2f} {unit}")
            print(f"  â€¢ ë²”ìœ„: {stats['min_sentence_length']} - {stats['max_sentence_length']} {unit}")
            print(f"  â€¢ ìš”ì²­ë‹¹ í‰ê·  ë¬¸ì¥ ìˆ˜: {stats['avg_sentences_per_request']:.2f}")
            
            # ë°ì´í„° ì €ì¥
            self.sentences_data = all_sentences
            self.requests_data = {
                'sentence_counts': request_sentence_counts,
                'avg_lengths': request_avg_lengths
            }
            
            return stats, responses
        else:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, responses
    
    def plot_distributions(self, unit='characters', save_path: str = None):
        """ë¶„í¬ ì‹œê°í™”"""
        if not self.sentences_data:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.log(f"ğŸ“Š ë¶„í¬ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('default')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'LLM ë¬¸ì¥ ê¸¸ì´ ë¶„ì„ ê²°ê³¼ ({unit})', fontsize=16, fontweight='bold')
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']
        
        # 1. ì „ì²´ ë¬¸ì¥ ê¸¸ì´ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)
        axes[0, 0].hist(self.sentences_data, bins=min(50, len(set(self.sentences_data))), 
                       alpha=0.7, color=colors[0], edgecolor='black', linewidth=0.5)
        axes[0, 0].set_title(f'ì „ì²´ ë¬¸ì¥ ê¸¸ì´ ë¶„í¬', fontsize=12, fontweight='bold')
        axes[0, 0].set_xlabel(f'ë¬¸ì¥ ê¸¸ì´ ({unit})')
        axes[0, 0].set_ylabel('ë¹ˆë„')
        axes[0, 0].grid(True, alpha=0.3)
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        mean_val = np.mean(self.sentences_data)
        median_val = np.median(self.sentences_data)
        axes[0, 0].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'í‰ê· : {mean_val:.1f}')
        axes[0, 0].axvline(median_val, color='orange', linestyle='--', alpha=0.8, label=f'ì¤‘ê°„ê°’: {median_val:.1f}')
        axes[0, 0].legend()
        
        # 2. ë¬¸ì¥ ê¸¸ì´ ë°•ìŠ¤í”Œë¡¯
        box_plot = axes[0, 1].boxplot(self.sentences_data, patch_artist=True)
        box_plot['boxes'][0].set_facecolor(colors[1])
        box_plot['boxes'][0].set_alpha(0.7)
        axes[0, 1].set_title(f'ë¬¸ì¥ ê¸¸ì´ ë°•ìŠ¤í”Œë¡¯', fontsize=12, fontweight='bold')
        axes[0, 1].set_ylabel(f'ë¬¸ì¥ ê¸¸ì´ ({unit})')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ìš”ì²­ë³„ ë¬¸ì¥ ìˆ˜ ë¶„í¬
        if self.requests_data.get('sentence_counts'):
            axes[1, 0].hist(self.requests_data['sentence_counts'], 
                           bins=min(30, len(set(self.requests_data['sentence_counts']))), 
                           alpha=0.7, color=colors[2], edgecolor='black', linewidth=0.5)
            axes[1, 0].set_title('ìš”ì²­ë³„ ë¬¸ì¥ ìˆ˜ ë¶„í¬', fontsize=12, fontweight='bold')
            axes[1, 0].set_xlabel('ìš”ì²­ë‹¹ ë¬¸ì¥ ìˆ˜')
            axes[1, 0].set_ylabel('ë¹ˆë„')
            axes[1, 0].grid(True, alpha=0.3)
            
            avg_count = np.mean(self.requests_data['sentence_counts'])
            axes[1, 0].axvline(avg_count, color='red', linestyle='--', alpha=0.8, 
                              label=f'í‰ê· : {avg_count:.1f}')
            axes[1, 0].legend()
        
        # 4. ìš”ì²­ë³„ í‰ê·  ë¬¸ì¥ ê¸¸ì´ ë¶„í¬
        if self.requests_data.get('avg_lengths'):
            axes[1, 1].hist(self.requests_data['avg_lengths'], 
                           bins=min(30, len(set(self.requests_data['avg_lengths']))), 
                           alpha=0.7, color=colors[3], edgecolor='black', linewidth=0.5)
            axes[1, 1].set_title(f'ìš”ì²­ë³„ í‰ê·  ë¬¸ì¥ ê¸¸ì´ ë¶„í¬', fontsize=12, fontweight='bold')
            axes[1, 1].set_xlabel(f'í‰ê·  ë¬¸ì¥ ê¸¸ì´ ({unit})')
            axes[1, 1].set_ylabel('ë¹ˆë„')
            axes[1, 1].grid(True, alpha=0.3)
            
            avg_length = np.mean(self.requests_data['avg_lengths'])
            axes[1, 1].axvline(avg_length, color='red', linestyle='--', alpha=0.8, 
                              label=f'í‰ê· : {avg_length:.1f}')
            axes[1, 1].legend()
        
        plt.tight_layout()
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ì°¨íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        else:
            plt.show()
        
        self.log("âœ… ì‹œê°í™” ì™„ë£Œ!")
    
    def generate_report(self, unit='characters', save_path: str = None):
        """ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if not self.sentences_data:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        report_lines = []
        
        # í—¤ë”
        header = f"LLM ë¬¸ì¥ ê¸¸ì´ ë¶„ì„ ë³´ê³ ì„œ ({unit} ê¸°ì¤€)"
        report_lines.append("="*70)
        report_lines.append(f"    {header}")
        report_lines.append("="*70)
        report_lines.append(f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ëª¨ë¸ ì •ë³´
        if self.model and self.current_model_name:
            report_lines.append(f"\nğŸ¤– ì‚¬ìš©ëœ ëª¨ë¸:")
            report_lines.append(f"  â€¢ ëª¨ë¸ëª…: {self.current_model_name}")
            report_lines.append(f"  â€¢ íŒŒë¼ë¯¸í„° ìˆ˜: {self.model.num_parameters():,}")
            report_lines.append(f"  â€¢ ëª¨ë¸ íƒ€ì…: {type(self.model).__name__}")
        
        # ê¸°ë³¸ í†µê³„
        report_lines.append(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
        report_lines.append(f"  â€¢ ì´ ë¬¸ì¥ ìˆ˜: {len(self.sentences_data):,}")
        report_lines.append(f"  â€¢ í‰ê·  ê¸¸ì´: {np.mean(self.sentences_data):.2f} {unit}")
        report_lines.append(f"  â€¢ ì¤‘ê°„ê°’: {np.median(self.sentences_data):.2f} {unit}")
        report_lines.append(f"  â€¢ í‘œì¤€í¸ì°¨: {np.std(self.sentences_data):.2f} {unit}")
        report_lines.append(f"  â€¢ ìµœì†Œ ê¸¸ì´: {min(self.sentences_data)} {unit}")
        report_lines.append(f"  â€¢ ìµœëŒ€ ê¸¸ì´: {max(self.sentences_data)} {unit}")
        
        # ë¶„ìœ„ìˆ˜ ì •ë³´
        report_lines.append(f"\nğŸ“ˆ ë¶„ìœ„ìˆ˜ ì •ë³´:")
        percentiles = [5, 10, 25, 50, 75, 90, 95, 99]
        for p in percentiles:
            value = np.percentile(self.sentences_data, p)
            report_lines.append(f"  â€¢ {p:2d}%: {value:6.1f} {unit}")
        
        # ê¸¸ì´ë³„ ë¶„í¬
        report_lines.append(f"\nğŸ“ ê¸¸ì´ë³„ ë¶„í¬:")
        if unit == 'characters':
            ranges = [(0, 10), (11, 30), (31, 50), (51, 100), (101, 200), (201, 500), (501, float('inf'))]
            labels = ['ê·¹ë„ë¡œ ì§§ìŒ', 'ë§¤ìš° ì§§ìŒ', 'ì§§ìŒ', 'ë³´í†µ', 'ê¹€', 'ë§¤ìš° ê¹€', 'ê·¹ë„ë¡œ ê¹€']
        elif unit == 'tokens':
            ranges = [(0, 5), (6, 10), (11, 20), (21, 50), (51, 100), (101, float('inf'))]
            labels = ['ê·¹ë„ë¡œ ì§§ìŒ', 'ë§¤ìš° ì§§ìŒ', 'ì§§ìŒ', 'ë³´í†µ', 'ê¹€', 'ë§¤ìš° ê¹€']
        else:  # words
            ranges = [(0, 3), (4, 5), (6, 10), (11, 20), (21, 50), (51, float('inf'))]
            labels = ['ê·¹ë„ë¡œ ì§§ìŒ', 'ë§¤ìš° ì§§ìŒ', 'ì§§ìŒ', 'ë³´í†µ', 'ê¹€', 'ë§¤ìš° ê¹€']
        
        for (min_len, max_len), label in zip(ranges, labels):
            if max_len == float('inf'):
                count = sum(1 for x in self.sentences_data if x >= min_len)
                percentage = count/len(self.sentences_data)*100
                report_lines.append(f"  â€¢ {label:10s} ({min_len:3d}+ {unit}): {count:5,}ê°œ ({percentage:5.1f}%)")
            else:
                count = sum(1 for x in self.sentences_data if min_len <= x <= max_len)
                percentage = count/len(self.sentences_data)*100
                report_lines.append(f"  â€¢ {label:10s} ({min_len:3d}-{max_len:3d} {unit}): {count:5,}ê°œ ({percentage:5.1f}%)")
        
        # ìš”ì²­ ê´€ë ¨ í†µê³„
        if self.requests_data.get('sentence_counts'):
            report_lines.append(f"\nğŸ“‹ ìš”ì²­ ê´€ë ¨ í†µê³„:")
            report_lines.append(f"  â€¢ ì´ ìš”ì²­ ìˆ˜: {len(self.requests_data['sentence_counts']):,}")
            report_lines.append(f"  â€¢ ìš”ì²­ë‹¹ í‰ê·  ë¬¸ì¥ ìˆ˜: {np.mean(self.requests_data['sentence_counts']):.2f}")
            report_lines.append(f"  â€¢ ìš”ì²­ë‹¹ ë¬¸ì¥ ìˆ˜ ë²”ìœ„: {min(self.requests_data['sentence_counts'])} - {max(self.requests_data['sentence_counts'])}")
            
            if self.requests_data.get('avg_lengths'):
                report_lines.append(f"  â€¢ ìš”ì²­ë³„ í‰ê·  ê¸¸ì´: {np.mean(self.requests_data['avg_lengths']):.2f} {unit}")
        
        # ë³´ê³ ì„œ ì¶œë ¥
        report_text = "\n".join(report_lines)
        print(report_text)
        
        # íŒŒì¼ ì €ì¥
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"\nğŸ’¾ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        return report_text
    
    def compare_models(self, model_names: List[str], test_prompts: List[str], 
                      unit='characters', max_length: int = 80):
        """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ë¶„ì„"""
        print(f"ğŸ”¬ ëª¨ë¸ ë¹„êµ ë¶„ì„")
        print("="*50)
        
        results = {}
        
        for i, model_name in enumerate(model_names):
            print(f"\nğŸ“Š ëª¨ë¸ {i+1}/{len(model_names)}: {model_name}")
            print("-" * 30)
            
            if self.load_local_model(model_name):
                stats, responses = self.analyze_generated_responses(
                    test_prompts,
                    unit=unit,
                    max_length=max_length,
                    show_responses=False
                )
                
                if stats:
                    results[model_name] = {
                        'stats': stats,
                        'responses': responses
                    }
                    print(f"âœ… {model_name} ë¶„ì„ ì™„ë£Œ")
                else:
                    print(f"{model_name} ë¶„ì„ ì‹¤íŒ¨")
            else:
                print(f"{model_name} ë¡œë“œ ì‹¤íŒ¨")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                del self.tokenizer
                del self.pipeline
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        # ë¹„êµ ê²°ê³¼ ì¶œë ¥
        if results:
            print(f"\nğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
            print("="*50)
            
            comparison_data = []
            for model_name, result in results.items():
                stats = result['stats']
                comparison_data.append({
                    'ëª¨ë¸': model_name,
                    f'í‰ê·  ê¸¸ì´ ({unit})': f"{stats['avg_sentence_length']:.2f}",
                    'ì´ ë¬¸ì¥ ìˆ˜': stats['total_sentences'],
                    'ìš”ì²­ë‹¹ ë¬¸ì¥ ìˆ˜': f"{stats['avg_sentences_per_request']:.2f}",
                    'í‘œì¤€í¸ì°¨': f"{stats['std_sentence_length']:.2f}"
                })
            
            # í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
            df = pd.DataFrame(comparison_data)
            print(df.to_string(index=False))
        
        return results


def create_argument_parser():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="ë¡œì»¬ LLM ë¬¸ì¥ ê¸¸ì´ ë¶„ì„ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ì„
  python %(prog)s --model microsoft/DialoGPT-small --prompts "ì•ˆë…•í•˜ì„¸ìš”" "íŒŒì´ì¬ì´ë€?"
  
  # ë°ì´í„°ì…‹ ë¶„ì„
  python %(prog)s --dataset OpenAssistant/oasst1 --unit words --sample-size 500
  
  # ëª¨ë¸ ë¹„êµ
  python %(prog)s --compare microsoft/DialoGPT-small google/flan-t5-small --prompts "Hello" "AIë€?"
  
  # íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ì½ê¸°
  python %(prog)s --model skt/kogpt2-base-v2 --prompts-file prompts.txt --output results/
        """
    )
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument('--model', '-m', type=str, 
                       help='ì‚¬ìš©í•  ëª¨ë¸ëª… (ì˜ˆ: microsoft/DialoGPT-small)')
    
    parser.add_argument('--dataset', '-d', type=str,
                       help='ë¶„ì„í•  ë°ì´í„°ì…‹ëª… (ì˜ˆ: OpenAssistant/oasst1)')
    
    parser.add_argument('--prompts', '-p', nargs='+', type=str,
                       help='ë¶„ì„í•  í”„ë¡¬í”„íŠ¸ë“¤')
    
    parser.add_argument('--prompts-file', type=str,
                       help='í”„ë¡¬í”„íŠ¸ê°€ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)')
    
    # ë¶„ì„ ì„¤ì •
    parser.add_argument('--unit', '-u', type=str, choices=['characters', 'words', 'tokens'], 
                       default='characters', help='ì¸¡ì • ë‹¨ìœ„ (ê¸°ë³¸ê°’: characters)')
    
    parser.add_argument('--max-length', type=int, default=100,
                       help='ìµœëŒ€ ìƒì„± ê¸¸ì´ (ê¸°ë³¸ê°’: 100)')
    
    parser.add_argument('--temperature', '-t', type=float, default=0.7,
                       help='Temperature ê°’ (ê¸°ë³¸ê°’: 0.7)')
    
    parser.add_argument('--sample-size', type=int, default=1000,
                       help='ë°ì´í„°ì…‹ ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ê°’: 1000)')
    
    parser.add_argument('--language', type=str,
                       help='ì–¸ì–´ í•„í„° (ì˜ˆ: ko, en)')
    
    parser.add_argument('--text-field', type=str,
                       help='ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸ í•„ë“œëª…')
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument('--output', '-o', type=str,
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    parser.add_argument('--no-plot', action='store_true',
                       help='ì‹œê°í™” ë¹„í™œì„±í™”')
    
    parser.add_argument('--no-report', action='store_true',
                       help='ë³´ê³ ì„œ ë¹„í™œì„±í™”')
    
    parser.add_argument('--show-responses', action='store_true',
                       help='ìƒì„±ëœ ì‘ë‹µ ì¶œë ¥')
    
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='ì§„í–‰ ìƒí™© ì¶œë ¥ ë¹„í™œì„±í™”')
    
    # ëª¨ë¸ ë¹„êµ
    parser.add_argument('--compare', nargs='+', type=str,
                       help='ë¹„êµí•  ëª¨ë¸ë“¤ ëª©ë¡')
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    parser.add_argument('--device', type=str, choices=['auto', 'cpu', 'cuda'], 
                       default='auto', help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: auto)')
    
    # ë„ì›€ë§
    parser.add_argument('--list-models', action='store_true',
                       help='ì¶”ì²œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥')
    
    parser.add_argument('--list-datasets', action='store_true',
                       help='ì¶”ì²œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥')
    
    return parser


def load_prompts_from_file(file_path: str) -> List[str]:
    """íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]
        return prompts
    except Exception as e:
        print(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # ë„ì›€ë§ ì˜µì…˜
    if args.list_models:
        analyzer = LocalLLMSentenceAnalyzer(verbose=False)
        print("ğŸ¤– ì¶”ì²œ ëª¨ë¸ ëª©ë¡:")
        print("="*50)
        for category, models in analyzer.recommended_models.items():
            print(f"\n{category}:")
            for model in models:
                print(f"  â€¢ {model}")
        return
    
    if args.list_datasets:
        print("ğŸ“Š ì¶”ì²œ ë°ì´í„°ì…‹ ëª©ë¡:")
        print("="*50)
        datasets = {
            "ëŒ€í™”í˜• ë°ì´í„°ì…‹": [
                "OpenAssistant/oasst1",
                "facebook/empathetic_dialogues",
                "daily_dialog",
            ],
            "QA ë°ì´í„°ì…‹": [
                "squad", "squad_v2", 
                "natural_questions",
                "ms_marco",
            ],
            "í•œêµ­ì–´ ë°ì´í„°ì…‹": [
                "squad_kor_v1",
                "klue",
                "korquad",
            ]
        }
        for category, dataset_list in datasets.items():
            print(f"\n{category}:")
            for dataset in dataset_list:
                print(f"  â€¢ {dataset}")
        return
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = LocalLLMSentenceAnalyzer(verbose=not args.quiet)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output:
        os.makedirs(args.output, exist_ok=True)
        plot_path = os.path.join(args.output, f"analysis_plot_{int(time.time())}.png")
        report_path = os.path.join(args.output, f"analysis_report_{int(time.time())}.txt")
    else:
        plot_path = None
        report_path = None
    
    # ëª¨ë¸ ë¹„êµ ëª¨ë“œ
    if args.compare:
        if not args.prompts and not args.prompts_file:
            print("ëª¨ë¸ ë¹„êµë¥¼ ìœ„í•´ --prompts ë˜ëŠ” --prompts-fileì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        if args.prompts_file:
            test_prompts = load_prompts_from_file(args.prompts_file)
        else:
            test_prompts = args.prompts
        
        if not test_prompts:
            print("ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ëª¨ë¸ ë¹„êµ ì‹¤í–‰
        results = analyzer.compare_models(
            args.compare, 
            test_prompts, 
            unit=args.unit,
            max_length=args.max_length
        )
        
        # ê²°ê³¼ ì €ì¥
        if args.output and results:
            comparison_path = os.path.join(args.output, f"model_comparison_{int(time.time())}.json")
            with open(comparison_path, 'w', encoding='utf-8') as f:
                # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                json_results = {}
                for model_name, result in results.items():
                    json_results[model_name] = {
                        'stats': result['stats'],
                        'sample_response': result['responses'][0] if result['responses'] else ""
                    }
                json.dump(json_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ë¹„êµ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {comparison_path}")
        
        return
    
    # ë‹¨ì¼ ëª¨ë¸ ë¶„ì„ ëª¨ë“œ
    if args.model:
        # ëª¨ë¸ ë¡œë“œ
        if not analyzer.load_local_model(args.model, device=args.device):
            print("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        # í”„ë¡¬í”„íŠ¸ ë¶„ì„
        if args.prompts or args.prompts_file:
            if args.prompts_file:
                prompts = load_prompts_from_file(args.prompts_file)
            else:
                prompts = args.prompts
            
            if not prompts:
                print("ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            stats, responses = analyzer.analyze_generated_responses(
                prompts,
                unit=args.unit,
                max_length=args.max_length,
                temperature=args.temperature,
                show_responses=args.show_responses
            )
            
            if stats:
                # ì‹œê°í™”
                if not args.no_plot:
                    analyzer.plot_distributions(unit=args.unit, save_path=plot_path)
                
                # ë³´ê³ ì„œ
                if not args.no_report:
                    analyzer.generate_report(unit=args.unit, save_path=report_path)
        else:
            print("ëª¨ë¸ ë¶„ì„ì„ ìœ„í•´ --prompts ë˜ëŠ” --prompts-fileì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë°ì´í„°ì…‹ ë¶„ì„ ëª¨ë“œ  
    elif args.dataset:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        if not analyzer.load_dataset(
            args.dataset,
            sample_size=args.sample_size,
            language=args.language
        ):
            print("ë°ì´í„°ì…‹ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„°ì…‹ ë¶„ì„
        stats = analyzer.analyze_existing_data(
            text_field=args.text_field,
            unit=args.unit
        )
        
        if stats:
            # ì‹œê°í™”
            if not args.no_plot:
                analyzer.plot_distributions(unit=args.unit, save_path=plot_path)
            
            # ë³´ê³ ì„œ
            if not args.no_report:
                analyzer.generate_report(unit=args.unit, save_path=report_path)
    
    else:
        print("--model ë˜ëŠ” --dataset ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        print("ë„ì›€ë§: python {} --help".format(sys.argv[0]))


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()