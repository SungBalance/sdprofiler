2025-01-19 11:31:11,940	INFO worker.py:1821 -- Started a local Ray instance.
[36m(GPUActor pid=137265)[0m /usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(GPUActor pid=137265)[0m   return torch.load(io.BytesIO(b))
run_do_work_remote
[36m(GPUActor pid=137265)[0m [Actor __init__] actor_id: 49996b1b3628d339be3bf43401000000,local_rank: 0,torch device: 0,CUDA_VISIBLE_DEVICES: 0,ray_assigned_gpus: []
[36m(GPUActor pid=137268)[0m [Actor __init__] actor_id: 080ccb04cb99375c1a3719bc01000000,local_rank: 1,torch device: 0,CUDA_VISIBLE_DEVICES: 1,ray_assigned_gpus: []
Before run_matmul
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from large pool |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from large pool |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from large pool |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from large pool |   1024 MiB |   1024 MiB |   1024 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       1    |       1    |       1    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       1    |       1    |       1    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       1    |       1    |       1    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[36m(GPUActor pid=137265)[0m Before run_matmul
[36m(GPUActor pid=137265)[0m |===========================================================================|
[36m(GPUActor pid=137265)[0m |                  PyTorch CUDA memory summary, device ID 0                 |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[36m(GPUActor pid=137265)[0m |===========================================================================|
[36m(GPUActor pid=137265)[0m |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Allocated memory      |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Active memory         |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Requested memory      |   1032 MiB |   1032 MiB |   1135 MiB | 105468 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |   1032 MiB |   1032 MiB |   1135 MiB | 105468 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | GPU reserved memory   |   1152 MiB |   1152 MiB |   1152 MiB |      0 B   |
[36m(GPUActor pid=137265)[0m |       from large pool |   1152 MiB |   1152 MiB |   1152 MiB |      0 B   |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Non-releasable memory |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 KiB |      0 KiB |      0 KiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Allocations           |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137265)[0m |       from large pool |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Active allocs         |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137265)[0m |       from large pool |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | GPU reserved segments |       5    |       5    |       5    |       0    |
[36m(GPUActor pid=137265)[0m |       from large pool |       5    |       5    |       5    |       0    |
[36m(GPUActor pid=137268)[0m /usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(GPUActor pid=137268)[0m   return torch.load(io.BytesIO(b))
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Non-releasable allocs |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137265)[0m |       from large pool |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Oversize allocations  |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Oversize GPU segments |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |===========================================================================|
[36m(GPUActor pid=137265)[0m 
[36m(GPUActor pid=137265)[0m After run_matmul
[36m(GPUActor pid=137265)[0m |===========================================================================|
[36m(GPUActor pid=137265)[0m |                  PyTorch CUDA memory summary, device ID 0                 |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[36m(GPUActor pid=137265)[0m |===========================================================================|
[36m(GPUActor pid=137265)[0m |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Allocated memory      |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Active memory         |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Requested memory      |   3080 MiB |   3080 MiB |   3183 MiB | 105468 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |   3080 MiB |   3080 MiB |   3183 MiB | 105468 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | GPU reserved memory   |   3200 MiB |   3200 MiB |   3200 MiB |      0 B   |
[36m(GPUActor pid=137265)[0m |       from large pool |   3200 MiB |   3200 MiB |   3200 MiB |      0 B   |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Non-releasable memory |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137265)[0m |       from large pool |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137265)[0m |       from small pool |      0 KiB |      0 KiB |      0 KiB |      0 KiB |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Allocations           |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137265)[0m |       from large pool |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Active allocs         |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137265)[0m |       from large pool |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | GPU reserved segments |       7    |       7    |       7    |       0    |
[36m(GPUActor pid=137265)[0m |       from large pool |       7    |       7    |       7    |       0    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Non-releasable allocs |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137265)[0m |       from large pool |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137265)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Oversize allocations  |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137265)[0m | Oversize GPU segments |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137265)[0m |===========================================================================|
[36m(GPUActor pid=137265)[0m 
[36m(GPUActor pid=137268)[0m Before run_matmul
[36m(GPUActor pid=137268)[0m |===========================================================================|
[36m(GPUActor pid=137268)[0m |                  PyTorch CUDA memory summary, device ID 0                 |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[36m(GPUActor pid=137268)[0m |===========================================================================|
[36m(GPUActor pid=137268)[0m |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Allocated memory      |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Active memory         |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |   1032 MiB |   1032 MiB |   1135 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Requested memory      |   1032 MiB |   1032 MiB |   1135 MiB | 105468 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |   1032 MiB |   1032 MiB |   1135 MiB | 105468 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | GPU reserved memory   |   1152 MiB |   1152 MiB |   1152 MiB |      0 B   |
[36m(GPUActor pid=137268)[0m |       from large pool |   1152 MiB |   1152 MiB |   1152 MiB |      0 B   |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Non-releasable memory |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 KiB |      0 KiB |      0 KiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Allocations           |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137268)[0m |       from large pool |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Active allocs         |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137268)[0m |       from large pool |       2    |       4    |       5    |       3    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | GPU reserved segments |       5    |       5    |       5    |       0    |
[36m(GPUActor pid=137268)[0m |       from large pool |       5    |       5    |       5    |       0    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Non-releasable allocs |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137268)[0m |       from large pool |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Oversize allocations  |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Oversize GPU segments |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |===========================================================================|
[36m(GPUActor pid=137268)[0m 
[36m(GPUActor pid=137268)[0m After run_matmul
[36m(GPUActor pid=137268)[0m |===========================================================================|
[36m(GPUActor pid=137268)[0m |                  PyTorch CUDA memory summary, device ID 0                 |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m |            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
[36m(GPUActor pid=137268)[0m |===========================================================================|
[36m(GPUActor pid=137268)[0m |        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Allocated memory      |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Active memory         |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |   3080 MiB |   3080 MiB |   3183 MiB | 105469 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Requested memory      |   3080 MiB |   3080 MiB |   3183 MiB | 105468 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |   3080 MiB |   3080 MiB |   3183 MiB | 105468 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | GPU reserved memory   |   3200 MiB |   3200 MiB |   3200 MiB |      0 B   |
[36m(GPUActor pid=137268)[0m |       from large pool |   3200 MiB |   3200 MiB |   3200 MiB |      0 B   |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Non-releasable memory |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137268)[0m |       from large pool |  12160 KiB |  17282 KiB |  17282 KiB |   5122 KiB |
[36m(GPUActor pid=137268)[0m |       from small pool |      0 KiB |      0 KiB |      0 KiB |      0 KiB |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Allocations           |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137268)[0m |       from large pool |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Active allocs         |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137268)[0m |       from large pool |       4    |       4    |       7    |       3    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | GPU reserved segments |       7    |       7    |       7    |       0    |
[36m(GPUActor pid=137268)[0m |       from large pool |       7    |       7    |       7    |       0    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Non-releasable allocs |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137268)[0m |       from large pool |       1    |       4    |       4    |       3    |
[36m(GPUActor pid=137268)[0m |       from small pool |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(GPUActor pid=137268)[0m | Oversize allocations  |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |---------------------------------------------------------------------------|
[36m(GPUActor pid=137268)[0m | Oversize GPU segments |       0    |       0    |       0    |       0    |
[36m(GPUActor pid=137268)[0m |===========================================================================|
[36m(GPUActor pid=137268)[0m 
After run_matmul
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from large pool |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from large pool |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from large pool |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from large pool |   3072 MiB |   3072 MiB |   3072 MiB |      0 B   |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       3    |       3    |       3    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       3    |       3    |       3    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       3    |       3    |       3    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

torch.Size([16384, 16384])
torch.Size([16384, 16384])
